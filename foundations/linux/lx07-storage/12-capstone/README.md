# 12 - ç»¼åˆé¡¹ç›®ï¼šå¼¹æ€§å­˜å‚¨æ¶æ„ï¼ˆCapstone: Resilient Storage Architectureï¼‰

> **ç›®æ ‡**ï¼šç»¼åˆè¿ç”¨å…¨è¯¾ç¨‹çŸ¥è¯†ï¼Œè®¾è®¡å¹¶å®æ–½åŒ…å«å†—ä½™ã€çµæ´»æ€§å’Œè‡ªåŠ¨åŒ–çš„ä¼ä¸šçº§å­˜å‚¨æ¶æ„  
> **å‰ç½®**ï¼šå®Œæˆ [01-11 å…¨éƒ¨è¯¾ç¨‹](../)  
> **æ—¶é—´**ï¼šâš¡ 40 åˆ†é’Ÿï¼ˆé€Ÿè¯»ï¼‰/ ğŸ”¬ 150 åˆ†é’Ÿï¼ˆå®Œæ•´å®æ“ï¼‰  
> **æˆæœ**ï¼šå®Œæ•´çš„å­˜å‚¨æ¶æ„è®¾è®¡æ–‡æ¡£ + å¯è¿è¡Œçš„é…ç½®è„šæœ¬ + è‡ªåŠ¨åŒ–å¤‡ä»½å’Œç›‘æ§  

---

## å°†å­¦åˆ°çš„å†…å®¹

1. è®¾è®¡åŒ…å«å†—ä½™å’Œçµæ´»æ€§çš„å­˜å‚¨æ¶æ„
2. å®æ–½ LVM on RAID æ–¹æ¡ˆï¼ˆç”Ÿäº§æ¨èï¼‰
3. é…ç½®è‡ªåŠ¨åŒ–å¤‡ä»½æµç¨‹ï¼ˆrsync + cronï¼‰
4. å»ºç«‹å­˜å‚¨å®¹é‡ç›‘æ§å’Œå‘Šè­¦

---

## é¡¹ç›®æ¦‚è¿°

åœ¨è¿™ä¸ªç»¼åˆé¡¹ç›®ä¸­ï¼Œä½ å°†æ‰®æ¼”ä¸€å Linux ç³»ç»Ÿç®¡ç†å‘˜ï¼Œä¸ºå…¬å¸çš„æ–‡ä»¶æœåŠ¡å™¨è®¾è®¡å’Œå®æ–½å­˜å‚¨æ–¹æ¡ˆã€‚

**ä¸šåŠ¡éœ€æ±‚**ï¼š

| éœ€æ±‚ | è¯´æ˜ |
|------|------|
| **å†—ä½™** | å•å—ç£ç›˜æ•…éšœä¸ä¸¢æ•°æ® |
| **çµæ´»** | å¯åœ¨çº¿æ‰©å±•å­˜å‚¨ç©ºé—´ |
| **å¤‡ä»½** | æ¯æ—¥è‡ªåŠ¨å¤‡ä»½åˆ°å¼‚åœ° |
| **ç›‘æ§** | å®¹é‡å‘Šè­¦ï¼Œæå‰é¢„è­¦ |

**æŠ€æœ¯é€‰å‹**ï¼šLVM on RAIDï¼ˆRAID æä¾›å†—ä½™ï¼ŒLVM æä¾›çµæ´»æ€§ï¼‰

---

## æ¶æ„é€‰æ‹©ï¼šLVM on RAID vs RAID on LVM

### ä¸¤ç§æ–¹æ¡ˆå¯¹æ¯”

<!-- DIAGRAM: lvm-raid-architectures -->
![LVM on RAID vs RAID on LVM](images/lvm-raid-architectures.png)

<details>
<summary>View ASCII source</summary>

```
æ–¹æ¡ˆ Aï¼šLVM on RAIDï¼ˆæ¨èï¼‰
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   Filesystem       â”‚        ext4 / XFS              â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   Logical Volume   â”‚  lv_data      â”‚   lv_backup    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   Volume Group     â”‚           vg_storage            â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   Physical Volume  â”‚         /dev/md0 (PV)           â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   RAID 1           â”‚    loop1    |    loop2         â”‚
                    â”‚    (disk1)  |    (disk2)       â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æ–¹æ¡ˆ Bï¼šRAID on LVMï¼ˆä¸æ¨èç”Ÿäº§ä½¿ç”¨ï¼‰
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   RAID 1           â”‚          /dev/md0              â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   Logical Volumes  â”‚    lv1 (VG1)  |  lv2 (VG2)     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   Volume Groups    â”‚      VG1      |      VG2       â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   Physical Volumes â”‚   PV (disk1)  |  PV (disk2)    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

</details>
<!-- /DIAGRAM -->

### ä¸ºä»€ä¹ˆé€‰æ‹© LVM on RAIDï¼Ÿ

| æ–¹é¢ | LVM on RAID | RAID on LVM |
|------|-------------|-------------|
| **ç®¡ç†å¤æ‚åº¦** | ä½ | é«˜ |
| **å†—ä½™æ¸…æ™°åº¦** | RAID å±‚ç»Ÿä¸€ç®¡ç† | å¤šä¸ª VG åˆ†æ•£ç®¡ç† |
| **æ‰©å±•æ€§** | æ·»åŠ ç£ç›˜åˆ° RAIDï¼ŒVG è‡ªåŠ¨æ‰©å±• | éœ€è¦åˆ†åˆ«æ‰©å±•å¤šä¸ª VG |
| **å¿«ç…§æ”¯æŒ** | LVM å¿«ç…§æ­£å¸¸ä½¿ç”¨ | å¿«ç…§è¦†ç›–æ‰€æœ‰æ•°æ® |
| **ç”Ÿäº§æ¨è** | **æ˜¯** | å¦ |

> **ç»“è®º**ï¼šç”Ÿäº§ç¯å¢ƒä¼˜å…ˆé€‰æ‹© LVM on RAIDã€‚æœ¬é¡¹ç›®é‡‡ç”¨æ­¤æ–¹æ¡ˆã€‚  

---

## å®éªŒç¯å¢ƒå‡†å¤‡

```bash
# åˆ›å»º 4 ä¸ªè™šæ‹Ÿç£ç›˜ï¼ˆç”¨äº RAID + çƒ­å¤‡ï¼‰
for i in 1 2 3 4; do
  fallocate -l 1G /tmp/disk$i.img
  sudo losetup /dev/loop$i /tmp/disk$i.img
done

# éªŒè¯
lsblk /dev/loop{1,2,3,4}

# åˆ›å»ºå¤‡ä»½ç›®æ ‡ç›®å½•ï¼ˆæ¨¡æ‹Ÿè¿œç¨‹å­˜å‚¨ï¼‰
sudo mkdir -p /backup
```

---

## Part 1 -- åˆ›å»º RAID é˜µåˆ—ï¼ˆ20 åˆ†é’Ÿï¼‰

### 1.1 åˆ›å»º RAID 1

```bash
# ä½¿ç”¨ loop1 å’Œ loop2 åˆ›å»º RAID 1
sudo mdadm --create /dev/md0 \
  --level=1 \
  --raid-devices=2 \
  /dev/loop1 /dev/loop2

# ç¡®è®¤åˆ›å»ºï¼ˆè¾“å…¥ yï¼‰
```

### 1.2 æ·»åŠ  Hot Spare

```bash
# loop3 ä½œä¸ºçƒ­å¤‡ç›˜
sudo mdadm --manage /dev/md0 --add-spare /dev/loop3

# éªŒè¯çŠ¶æ€
cat /proc/mdstat
sudo mdadm --detail /dev/md0
```

é¢„æœŸè¾“å‡ºï¼š
```
md0 : active raid1 loop3[2](S) loop2[1] loop1[0]
      1046528 blocks super 1.2 [2/2] [UU]
```

> **è¯´æ˜**ï¼š`(S)` è¡¨ç¤º Spare çŠ¶æ€ã€‚  

### 1.3 æŒä¹…åŒ– RAID é…ç½®

```bash
# ä¿å­˜ RAID é…ç½®
sudo mdadm --detail --scan | sudo tee -a /etc/mdadm/mdadm.conf

# æŸ¥çœ‹ä¿å­˜çš„é…ç½®
cat /etc/mdadm/mdadm.conf
```

---

## Part 2 -- åˆ›å»º LVM on RAIDï¼ˆ25 åˆ†é’Ÿï¼‰

### 2.1 åœ¨ RAID ä¸Šåˆ›å»º PV

```bash
# å°† RAID è®¾å¤‡åˆå§‹åŒ–ä¸º LVM ç‰©ç†å·
sudo pvcreate /dev/md0

# éªŒè¯
sudo pvs
```

### 2.2 åˆ›å»º VG

```bash
# åˆ›å»º Volume Group
sudo vgcreate vg_storage /dev/md0

# éªŒè¯
sudo vgs
```

### 2.3 åˆ›å»º LV

```bash
# åˆ›å»ºæ•°æ®å·ï¼ˆ600MBï¼‰
sudo lvcreate -L 600M -n lv_data vg_storage

# åˆ›å»ºå¤‡ä»½æš‚å­˜å·ï¼ˆ300MBï¼‰
sudo lvcreate -L 300M -n lv_staging vg_storage

# é¢„ç•™ç©ºé—´ç”¨äºå°†æ¥æ‰©å±•
# éªŒè¯
sudo lvs
```

é¢„æœŸè¾“å‡ºï¼š
```
  LV         VG         Attr       LSize   Pool Origin Data%  Meta%
  lv_data    vg_storage -wi-a----- 600.00m
  lv_staging vg_storage -wi-a----- 300.00m
```

### 2.4 åˆ›å»ºæ–‡ä»¶ç³»ç»Ÿ

```bash
# æ•°æ®å·ä½¿ç”¨ ext4ï¼ˆæ”¯æŒæ”¶ç¼©ï¼‰
sudo mkfs.ext4 /dev/vg_storage/lv_data

# æš‚å­˜å·ä½¿ç”¨ XFSï¼ˆé«˜ååé‡ï¼‰
sudo mkfs.xfs /dev/vg_storage/lv_staging
```

### 2.5 æŒ‚è½½

```bash
# åˆ›å»ºæŒ‚è½½ç‚¹
sudo mkdir -p /data /staging

# æŒ‚è½½
sudo mount /dev/vg_storage/lv_data /data
sudo mount /dev/vg_storage/lv_staging /staging

# éªŒè¯
df -h /data /staging
```

---

## Part 3 -- fstab é…ç½®ï¼ˆ15 åˆ†é’Ÿï¼‰

### 3.1 è·å– UUID

```bash
# æŸ¥çœ‹ LV çš„ UUID
sudo blkid /dev/vg_storage/lv_data
sudo blkid /dev/vg_storage/lv_staging
```

### 3.2 é…ç½® fstab

```bash
# å¤‡ä»½åŸå§‹ fstab
sudo cp /etc/fstab /etc/fstab.backup

# æ·»åŠ æŒ‚è½½é…ç½®ï¼ˆä½¿ç”¨ UUIDï¼‰
# æ ¼å¼ï¼šUUID=xxx  æŒ‚è½½ç‚¹  ç±»å‹  é€‰é¡¹  dump  pass
```

**ç¼–è¾‘ /etc/fstabï¼Œæ·»åŠ ä»¥ä¸‹å†…å®¹**ï¼š

```
# Storage Project - LVM on RAID
# Created: $(date)

# Data volume - ext4, nofail for non-critical
UUID=<lv_dataçš„UUID>    /data      ext4  defaults,nofail  0  2

# Staging volume - XFS, nofail
UUID=<lv_stagingçš„UUID> /staging   xfs   defaults,nofail  0  2
```

**å…³é”®é€‰é¡¹**ï¼š

| é€‰é¡¹ | ä½œç”¨ |
|------|------|
| `nofail` | è®¾å¤‡ä¸å­˜åœ¨æ—¶ä¸é˜»å¡å¯åŠ¨ |
| `defaults` | rw, suid, dev, exec, auto, nouser, async |
| `0` (dump) | ä¸éœ€è¦ dump å¤‡ä»½ |
| `2` (pass) | éæ ¹åˆ†åŒºï¼Œç¬¬äºŒè½® fsck |

### 3.3 éªŒè¯ fstab

```bash
# å¸è½½åé‡æ–°æŒ‚è½½æµ‹è¯•
sudo umount /data /staging
sudo mount -a

# å¦‚æœæ²¡æœ‰é”™è¯¯ï¼Œè¯´æ˜é…ç½®æ­£ç¡®
df -h /data /staging
```

> **Critical**ï¼š**æ°¸è¿œä¸è¦è·³è¿‡ `mount -a` æµ‹è¯•ï¼** fstab é”™è¯¯ä¼šå¯¼è‡´æœåŠ¡å™¨æ— æ³•å¯åŠ¨ã€‚  

---

## Part 4 -- è‡ªåŠ¨åŒ–å¤‡ä»½ï¼ˆ30 åˆ†é’Ÿï¼‰

### 4.1 å¤‡ä»½è„šæœ¬

åˆ›å»º `/usr/local/bin/storage-backup.sh`ï¼š

```bash
#!/bin/bash
# storage-backup.sh - Automated backup with LVM snapshot
#
# This script:
# 1. Creates LVM snapshot for consistency
# 2. Mounts snapshot read-only
# 3. Syncs to backup destination
# 4. Removes snapshot after completion

set -e

# Configuration
SOURCE_LV="/dev/vg_storage/lv_data"
SNAP_NAME="snap_backup"
SNAP_SIZE="100M"
SNAP_MOUNT="/mnt/snap_backup"
BACKUP_DEST="/backup/data"
LOG_FILE="/var/log/storage-backup.log"

# Logging function
log() {
  echo "$(date '+%Y-%m-%d %H:%M:%S') - $1" | tee -a "$LOG_FILE"
}

# Cleanup function
cleanup() {
  log "Cleaning up..."
  umount "$SNAP_MOUNT" 2>/dev/null || true
  lvremove -f "/dev/vg_storage/$SNAP_NAME" 2>/dev/null || true
}

# Set trap for cleanup
trap cleanup EXIT

# Main backup process
main() {
  log "=== Starting backup ==="

  # Create snapshot
  log "Creating snapshot..."
  lvcreate -s -L "$SNAP_SIZE" -n "$SNAP_NAME" "$SOURCE_LV"

  # Mount snapshot read-only
  log "Mounting snapshot..."
  mkdir -p "$SNAP_MOUNT"
  mount -o ro "/dev/vg_storage/$SNAP_NAME" "$SNAP_MOUNT"

  # Ensure backup destination exists
  mkdir -p "$BACKUP_DEST"

  # Rsync with incremental backup
  log "Starting rsync..."
  rsync -av --delete \
    "$SNAP_MOUNT/" \
    "$BACKUP_DEST/"

  log "=== Backup completed successfully ==="
}

# Run main function
main "$@"
```

### 4.2 è®¾ç½®æƒé™å¹¶æµ‹è¯•

```bash
# è®¾ç½®å¯æ‰§è¡Œæƒé™
sudo chmod +x /usr/local/bin/storage-backup.sh

# åˆ›å»ºæµ‹è¯•æ•°æ®
sudo mkdir -p /data/projects
echo "Project data v1" | sudo tee /data/projects/readme.txt

# æ‰‹åŠ¨è¿è¡Œæµ‹è¯•
sudo /usr/local/bin/storage-backup.sh

# éªŒè¯å¤‡ä»½
ls -la /backup/data/
cat /backup/data/projects/readme.txt
```

### 4.3 é…ç½® Cron å®šæ—¶ä»»åŠ¡

```bash
# ç¼–è¾‘ root çš„ crontab
sudo crontab -e
```

æ·»åŠ ä»¥ä¸‹è¡Œï¼ˆæ¯å¤©å‡Œæ™¨ 2 ç‚¹æ‰§è¡Œï¼‰ï¼š

```cron
# Storage backup - daily at 2:00 AM
0 2 * * * /usr/local/bin/storage-backup.sh >> /var/log/storage-backup.log 2>&1
```

> **æ—¥æœ¬ IT è¿ç»´å®è·µ**ï¼šå¤‡ä»½çª—å£ï¼ˆãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ï¼‰é€šå¸¸é€‰æ‹©ä¸šåŠ¡ä½å³°æœŸï¼Œå¦‚å‡Œæ™¨ 2-5 ç‚¹ã€‚  

---

## Part 5 -- å®¹é‡ç›‘æ§è„šæœ¬ï¼ˆ25 åˆ†é’Ÿï¼‰

### 5.1 ç›‘æ§è„šæœ¬

åˆ›å»º `/usr/local/bin/storage-monitor.sh`ï¼š

```bash
#!/bin/bash
# storage-monitor.sh - Storage capacity monitoring
#
# Checks:
# 1. Filesystem usage (df)
# 2. Inode usage (df -i)
# 3. LVM snapshot status
# 4. RAID status

# Configuration
WARN_THRESHOLD=80
CRIT_THRESHOLD=90
MAILTO="admin@example.com"

# Output buffer
ALERTS=""

# Check filesystem usage
check_filesystem() {
  echo "=== Filesystem Usage ==="
  df -h /data /staging 2>/dev/null | tail -n +2 | while read fs size used avail pct mount; do
    pct_num=${pct%\%}
    if (( pct_num >= CRIT_THRESHOLD )); then
      ALERTS="${ALERTS}CRITICAL: $mount at ${pct} usage\n"
      echo "CRITICAL: $mount is at ${pct}"
    elif (( pct_num >= WARN_THRESHOLD )); then
      ALERTS="${ALERTS}WARNING: $mount at ${pct} usage\n"
      echo "WARNING: $mount is at ${pct}"
    else
      echo "OK: $mount is at ${pct}"
    fi
  done
}

# Check inode usage
check_inodes() {
  echo ""
  echo "=== Inode Usage ==="
  df -i /data /staging 2>/dev/null | tail -n +2 | while read fs inodes iused ifree ipct mount; do
    pct_num=${ipct%\%}
    if (( pct_num >= CRIT_THRESHOLD )); then
      echo "CRITICAL: $mount inodes at ${ipct}"
    elif (( pct_num >= WARN_THRESHOLD )); then
      echo "WARNING: $mount inodes at ${ipct}"
    else
      echo "OK: $mount inodes at ${ipct}"
    fi
  done
}

# Check LVM snapshots
check_snapshots() {
  echo ""
  echo "=== LVM Snapshots ==="
  lvs --noheadings -o lv_name,data_percent,origin 2>/dev/null | while read lv pct origin; do
    if [[ -n "$origin" ]]; then
      pct_num=${pct%.*}
      if (( pct_num >= 80 )); then
        echo "WARNING: Snapshot $lv at ${pct}% - consider removing"
      else
        echo "OK: Snapshot $lv at ${pct}%"
      fi
    fi
  done
}

# Check RAID status
check_raid() {
  echo ""
  echo "=== RAID Status ==="
  if [[ -f /proc/mdstat ]]; then
    if grep -q '\[U_\]\|\_U\]\|\[_\]' /proc/mdstat; then
      echo "CRITICAL: RAID degraded!"
      cat /proc/mdstat
    else
      echo "OK: RAID healthy"
      grep "^md" /proc/mdstat
    fi
  else
    echo "INFO: No software RAID configured"
  fi
}

# Main
main() {
  echo "Storage Health Check - $(date)"
  echo "================================"
  check_filesystem
  check_inodes
  check_snapshots
  check_raid
  echo ""
  echo "Check completed."
}

main "$@"
```

### 5.2 æµ‹è¯•ç›‘æ§è„šæœ¬

```bash
# è®¾ç½®æƒé™
sudo chmod +x /usr/local/bin/storage-monitor.sh

# è¿è¡Œæµ‹è¯•
sudo /usr/local/bin/storage-monitor.sh
```

é¢„æœŸè¾“å‡ºï¼š
```
Storage Health Check - Sat Jan  4 14:30:00 JST 2026
================================
=== Filesystem Usage ===
OK: /data is at 5%
OK: /staging is at 1%

=== Inode Usage ===
OK: /data inodes at 1%
OK: /staging inodes at 1%

=== LVM Snapshots ===

=== RAID Status ===
OK: RAID healthy
md0 : active raid1 loop3[2](S) loop2[1] loop1[0]

Check completed.
```

### 5.3 é…ç½®å®šæ—¶ç›‘æ§

```bash
# æ·»åŠ åˆ° crontabï¼ˆæ¯å°æ—¶æ£€æŸ¥ï¼‰
sudo crontab -e
```

æ·»åŠ ï¼š
```cron
# Storage monitoring - every hour
0 * * * * /usr/local/bin/storage-monitor.sh >> /var/log/storage-monitor.log 2>&1
```

---

## è®¾è®¡æ–‡æ¡£æ¨¡æ¿

å®Œæˆå®æ–½åï¼Œæ’°å†™è®¾è®¡æ–‡æ¡£ï¼ˆè¨­è¨ˆæ›¸ï¼‰ï¼š

```markdown
# å­˜å‚¨æ¶æ„è®¾è®¡æ–‡æ¡£
# Storage Architecture Design Document

## 1. æ¦‚è¦ï¼ˆOverviewï¼‰

| é¡¹ç›® | å†…å®¹ |
|------|------|
| æœåŠ¡å™¨å | server-prod-01 |
| è®¾è®¡æ—¥æœŸ | 2026-01-04 |
| è®¾è®¡è€… | [Your Name] |
| ç‰ˆæœ¬ | v1.0 |

## 2. æ¶æ„ï¼ˆArchitectureï¼‰

é‡‡ç”¨ LVM on RAID æ¶æ„ï¼š
- RAID å±‚ï¼šRAID 1 (é•œåƒ) + 1 Hot Spare
- LVM å±‚ï¼š2 ä¸ª Logical Volumes
- æ–‡ä»¶ç³»ç»Ÿï¼šext4 (data), XFS (staging)

## 3. ç£ç›˜é…ç½®ï¼ˆDisk Configurationï¼‰

| è®¾å¤‡ | è§’è‰² | å®¹é‡ |
|------|------|------|
| /dev/sdb | RAID 1 æˆå‘˜ | 1TB |
| /dev/sdc | RAID 1 æˆå‘˜ | 1TB |
| /dev/sdd | Hot Spare | 1TB |

## 4. LVM é…ç½®ï¼ˆLVM Configurationï¼‰

| VG | LV | å¤§å° | æŒ‚è½½ç‚¹ | æ–‡ä»¶ç³»ç»Ÿ |
|----|----|----|--------|----------|
| vg_storage | lv_data | 600G | /data | ext4 |
| vg_storage | lv_staging | 300G | /staging | XFS |
| (é¢„ç•™) | - | 100G | - | - |

## 5. å¤‡ä»½ç­–ç•¥ï¼ˆBackup Strategyï¼‰

- æ–¹å¼ï¼šLVM Snapshot + rsync
- é¢‘ç‡ï¼šæ¯æ—¥ 02:00
- ä¿ç•™ï¼š7 å¤©
- ç›®æ ‡ï¼š/backup (NFS mount)

## 6. ç›‘æ§é…ç½®ï¼ˆMonitoringï¼‰

- å®¹é‡å‘Šè­¦ï¼š80% WARNING, 90% CRITICAL
- RAID çŠ¶æ€ï¼šhourly check
- æ—¥å¿—ä½ç½®ï¼š/var/log/storage-*.log

## 7. æ¢å¤æ‰‹é †ï¼ˆRecovery Proceduresï¼‰

### 7.1 RAID é™çº§æ¢å¤
1. ç¡®è®¤çŠ¶æ€ï¼šcat /proc/mdstat
2. è¯†åˆ«æ•…éšœç›˜ï¼šmdadm --detail /dev/md0
3. ç§»é™¤æ•…éšœç›˜ï¼šmdadm --fail --remove
4. æ›´æ¢ç‰©ç†ç£ç›˜
5. æ·»åŠ æ–°ç›˜ï¼šmdadm --add
6. ç›‘æ§é‡å»ºï¼šwatch cat /proc/mdstat

### 7.2 æ•°æ®æ¢å¤
1. ä» /backup ä½¿ç”¨ rsync æ¢å¤
2. å¦‚éœ€ç‰¹å®šæ—¶é—´ç‚¹ï¼Œä½¿ç”¨å¢é‡å¤‡ä»½
```

---

## èŒåœºå°è´´å£«ï¼ˆJapan IT Contextï¼‰

### è®¾è®¡ä¹¦ï¼ˆè¨­è¨ˆæ›¸ï¼‰çš„é‡è¦æ€§

åœ¨æ—¥æœ¬ IT ä¼ä¸šï¼Œ**è¨­è¨ˆæ›¸**ï¼ˆè®¾è®¡æ–‡æ¡£ï¼‰æ˜¯å¿…é¡»çš„ï¼š

| æ–‡æ¡£ç±»å‹ | æ—¥è¯­ | ç”¨é€” |
|----------|------|------|
| åŸºæœ¬è¨­è¨ˆæ›¸ | ãã»ã‚“ã›ã£ã‘ã„ã—ã‚‡ | æ•´ä½“æ¶æ„ |
| è©³ç´°è¨­è¨ˆæ›¸ | ã—ã‚‡ã†ã•ã„ã›ã£ã‘ã„ã—ã‚‡ | å…·ä½“é…ç½® |
| é‹ç”¨æ‰‹é †æ›¸ | ã†ã‚“ã‚ˆã†ã¦ã˜ã‚…ã‚“ã—ã‚‡ | æ—¥å¸¸æ“ä½œ |
| éšœå®³å¯¾å¿œæ‰‹é † | ã—ã‚‡ã†ãŒã„ãŸã„ãŠã†ã¦ã˜ã‚…ã‚“ | æ•…éšœæ¢å¤ |

### é¢è¯•åŠ åˆ†é¡¹

**Q: ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸è¨­è¨ˆã§é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã¯ä½•ã§ã™ã‹ï¼Ÿ**

A: ä¸‰ã¤ã®ãƒã‚¤ãƒ³ãƒˆãŒã‚ã‚Šã¾ã™ï¼š

1. **å†—é•·æ€§ï¼ˆRedundancyï¼‰**ï¼šRAID ã«ã‚ˆã‚‹è€éšœå®³æ€§ã®ç¢ºä¿
2. **æ‹¡å¼µæ€§ï¼ˆScalabilityï¼‰**ï¼šLVM ã«ã‚ˆã‚‹ã‚ªãƒ³ãƒ©ã‚¤ãƒ³æ‹¡å¼µã®å¯èƒ½æ€§
3. **é‹ç”¨æ€§ï¼ˆOperabilityï¼‰**ï¼šç›£è¦–ã€ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã€å¾©æ—§æ‰‹é †ã®æ•´å‚™

ç‰¹ã«æœ¬ç•ªç’°å¢ƒã§ã¯ã€ãƒ›ãƒƒãƒˆã‚¹ãƒšã‚¢ã®è¨­å®šã¨è‡ªå‹•ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®å®Ÿè£…ãŒé‡è¦ã§ã™ã€‚

---

## äº¤ä»˜ç‰©æ£€æŸ¥æ¸…å•

å®Œæˆé¡¹ç›®åï¼Œç¡®è®¤ä»¥ä¸‹äº¤ä»˜ç‰©ï¼š

### é…ç½®æˆæœ

- [ ] RAID 1 é˜µåˆ—åˆ›å»ºå®Œæˆï¼ŒçŠ¶æ€ä¸º [UU]
- [ ] Hot Spare å·²é…ç½®
- [ ] mdadm.conf å·²æ›´æ–°
- [ ] LVM on RAID åˆ›å»ºå®Œæˆï¼ˆvg_storage, lv_data, lv_stagingï¼‰
- [ ] fstab ä½¿ç”¨ UUID é…ç½®ï¼ŒåŒ…å« nofail
- [ ] mount -a æµ‹è¯•é€šè¿‡

### è‡ªåŠ¨åŒ–è„šæœ¬

- [ ] `/usr/local/bin/storage-backup.sh` å¯æ‰§è¡Œä¸”æµ‹è¯•é€šè¿‡
- [ ] `/usr/local/bin/storage-monitor.sh` å¯æ‰§è¡Œä¸”æµ‹è¯•é€šè¿‡
- [ ] Cron ä»»åŠ¡å·²é…ç½®ï¼ˆå¤‡ä»½ + ç›‘æ§ï¼‰

### æ–‡æ¡£

- [ ] å­˜å‚¨æ¶æ„è®¾è®¡æ–‡æ¡£å·²å®Œæˆ
- [ ] åŒ…å« RAIDã€LVMã€å¤‡ä»½ã€ç›‘æ§é…ç½®
- [ ] åŒ…å«æ¢å¤æ‰‹é †

---

## è¯¾ç¨‹å®Œæˆæ€»ç»“

æ­å–œï¼ä½ å·²å®Œæˆ Linux å­˜å‚¨ç®¡ç†å…¨éƒ¨ 12 è¯¾ã€‚

### æŠ€èƒ½æ€»è§ˆ

| æ¨¡å— | æŒæ¡æŠ€èƒ½ |
|------|----------|
| **åŸºç¡€** | å—è®¾å¤‡å‘½åã€UUIDã€fstab |
| **åˆ†åŒº** | fdisk/gdiskã€GPT/MBR |
| **æ–‡ä»¶ç³»ç»Ÿ** | ext4/XFS é€‰æ‹©ã€mkfsã€æŒ‚è½½ |
| **LVM** | PV/VG/LV åˆ›å»ºã€æ‰©å±•ã€å¿«ç…§ |
| **RAID** | çº§åˆ«é€‰æ‹©ã€mdadm æ“ä½œã€é™çº§æ¢å¤ |
| **è¿ç»´** | å¤‡ä»½ç­–ç•¥ã€å®¹é‡ç›‘æ§ã€æ•…éšœå¤„ç† |

### è®¤è¯å¯¹æ ‡

| è€ƒè¯• | è¦†ç›–å†…å®¹ |
|------|----------|
| **LPIC-2** | 203.1, 203.2, 203.3 (Storage) |
| **RHCSA** | Configure local storage, LVM, filesystems |

---

## å®éªŒæ¸…ç†

```bash
# å¸è½½æ–‡ä»¶ç³»ç»Ÿ
sudo umount /data /staging

# åˆ é™¤ LVM ç»“æ„
sudo lvremove -f /dev/vg_storage/lv_data
sudo lvremove -f /dev/vg_storage/lv_staging
sudo vgremove vg_storage
sudo pvremove /dev/md0

# åœæ­¢ RAID
sudo mdadm --stop /dev/md0
sudo mdadm --zero-superblock /dev/loop{1,2,3,4} 2>/dev/null

# é‡Šæ”¾ loop è®¾å¤‡
for i in 1 2 3 4; do
  sudo losetup -d /dev/loop$i 2>/dev/null
done

# åˆ é™¤æ¨¡æ‹Ÿç£ç›˜
rm -f /tmp/disk{1,2,3,4}.img

# åˆ é™¤å¤‡ä»½ç›®å½•
sudo rm -rf /backup

# åˆ é™¤è„šæœ¬ï¼ˆå¯é€‰ï¼‰
sudo rm -f /usr/local/bin/storage-backup.sh
sudo rm -f /usr/local/bin/storage-monitor.sh
```

---

## ä¸‹ä¸€æ­¥å­¦ä¹ 

å®Œæˆå­˜å‚¨ç®¡ç†è¯¾ç¨‹åï¼Œæ¨èç»§ç»­å­¦ä¹ ï¼š

| è¯¾ç¨‹ | å†…å®¹ | å…³è” |
|------|------|------|
| **LX09-PERFORMANCE** | I/O æ€§èƒ½åˆ†æ | iostat, iotop, fio |
| **LX10-TROUBLESHOOTING** | å­˜å‚¨æ•…éšœæ’æŸ¥ | ç»¼åˆè¯Šæ–­æŠ€èƒ½ |

---

## å»¶ä¼¸é˜…è¯»

- [Red Hat: LVM Administrator's Guide](https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/8/html-single/configuring_and_managing_logical_volumes/)
- [Arch Wiki: LVM](https://wiki.archlinux.org/title/LVM)
- [Linux RAID Wiki](https://raid.wiki.kernel.org/)
- ä¸Šä¸€è¯¾ï¼š[11 - æ–‡ä»¶ç³»ç»Ÿç»´æŠ¤](../11-filesystem-maintenance/) -- fsck, xfs_repair, inode è¯Šæ–­

---

## ç³»åˆ—å¯¼èˆª

[<-- 11 - æ–‡ä»¶ç³»ç»Ÿç»´æŠ¤](../11-filesystem-maintenance/) | [ç³»åˆ—é¦–é¡µ](../) | **è¯¾ç¨‹å®Œæˆ**
