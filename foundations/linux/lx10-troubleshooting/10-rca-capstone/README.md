# 10 - Capstone（RCA Capstone）

> **目标**：完成一个真实的生产事故模拟，从发现到恢复到撰写完整的 RCA 报告  
> **前置**：LX10 前 9 课全部内容  
> **时间**：⚡ 45 分钟（速读）/ 🔬 180 分钟（完整实操）  
> **核心理念**：故障排查不是"修好了就完了"，而是"让它不再发生"  

---

## 将学到的内容

1. 应用 Five Whys 方法论进行根因分析
2. 撰写日本企业标准的 障害報告書
3. 区分 暫定対応 和 恒久対策
4. 完成模拟生产事故的全流程处理
5. 理解 Blameless 文化的重要性

---

## 先跑起来！（10 分钟）

> 在学习 RCA 方法论之前，先看一个真实的事故场景。  
> 你将扮演 On-call 工程师，面对一个正在发生的生产事故。  

**场景：凌晨 3:00，你被告警叫醒**

```
[CRITICAL] Production API - Response Time > 5s
[CRITICAL] Production API - 5xx Error Rate > 50%
[WARNING] Database - CPU Usage > 90%
[WARNING] App Server - Disk I/O Wait > 80%
```

**问题**：
- 用户投诉：App 打不开，一直转圈
- 告警风暴：同时收到 4 个告警
- 时间紧迫：SLA 要求 99.9%，每分钟都在损失信誉

**你的第一反应是什么？**

A) 重启所有服务，先恢复再说
B) 先采集证据，然后按优先级排查
C) 看哪个告警最严重就先处理哪个
D) 等资深同事醒来一起看

**正确答案是 B**。

在本课中，你将学习如何系统性地处理这类复杂事故，并产出一份专业的 障害報告書。

---

## Step 1 -- Five Whys 方法论（30 分钟）

### 1.1 什么是 Five Whys？

Five Whys（5 个为什么）是丰田生产系统（Toyota Production System）发明的根因分析方法。核心思想很简单：

> **连续问 5 次"为什么"，直到找到系统/流程层面的根本原因。**  

<!-- DIAGRAM: five-whys-concept -->
```
┌──────────────────────────────────────────────────────────────────┐
│                    Five Whys 方法论                               │
├──────────────────────────────────────────────────────────────────┤
│                                                                  │
│   症状：API 返回 500 错误                                         │
│                                                                  │
│   ┌───────────────────────────────────────────────────────────┐ │
│   │ Why 1: 为什么 API 返回 500？                               │ │
│   │        → 因为应用无法写入数据库                             │ │
│   ├───────────────────────────────────────────────────────────┤ │
│   │ Why 2: 为什么无法写入数据库？                              │ │
│   │        → 因为磁盘空间满了                                   │ │
│   ├───────────────────────────────────────────────────────────┤ │
│   │ Why 3: 为什么磁盘空间满了？                                │ │
│   │        → 因为日志文件无限增长                               │ │
│   ├───────────────────────────────────────────────────────────┤ │
│   │ Why 4: 为什么日志文件无限增长？                            │ │
│   │        → 因为没有配置 logrotate                             │ │
│   ├───────────────────────────────────────────────────────────┤ │
│   │ Why 5: 为什么没有配置 logrotate？                          │ │
│   │        → 因为部署流程没有日志管理检查项                     │ │
│   └───────────────────────────────────────────────────────────┘ │
│                                                                  │
│   根因：部署流程缺少日志管理检查 ← 系统/流程层面                 │
│   预防：在部署检查清单中添加日志配置验证                         │
│                                                                  │
└──────────────────────────────────────────────────────────────────┘
```
<!-- /DIAGRAM -->

### 1.2 Five Whys 的核心规则

| 规则 | 说明 | 例子 |
|------|------|------|
| **停在系统层面** | 不要停在个人失误 | 错："小王忘了配置"<br>对："部署流程没有配置检查" |
| **区分触发器和根因** | 触发器是"什么导致了症状"<br>根因是"为什么系统无法应对" | 触发器：流量激增<br>根因：没有自动扩容 |
| **可以不是 5 次** | 3-7 次都正常，重点是达到系统层面 | 简单问题可能 3 次就够 |
| **一个分支一个根因** | 复杂问题可能有多个根因分支 | 技术根因 + 流程根因 |

### 1.3 触发器 vs 根因

这是 RCA 中最常见的混淆点：

<!-- DIAGRAM: trigger-vs-root-cause -->
```
┌──────────────────────────────────────────────────────────────────┐
│                    触发器 vs 根因                                 │
├──────────────────────────────────────────────────────────────────┤
│                                                                  │
│    触发器 (Trigger)              根因 (Root Cause)               │
│    ┌─────────────────┐           ┌─────────────────┐            │
│    │ 什么导致了      │           │ 为什么系统      │            │
│    │ 症状发生？      │           │ 无法应对？      │            │
│    └────────┬────────┘           └────────┬────────┘            │
│             │                             │                      │
│             ▼                             ▼                      │
│    ┌─────────────────┐           ┌─────────────────┐            │
│    │ • 流量激增      │           │ • 没有自动扩容  │            │
│    │ • 磁盘满        │           │ • 没有容量监控  │            │
│    │ • 配置错误      │           │ • 没有配置审查  │            │
│    │ • 上游服务挂了  │           │ • 没有熔断机制  │            │
│    └─────────────────┘           └─────────────────┘            │
│                                                                  │
│    关键区别：                                                    │
│    • 触发器是"外部事件"，往往不可控                              │
│    • 根因是"内部缺陷"，可以通过改进预防                          │
│                                                                  │
│    错误示例：                                                    │
│    "根因是流量激增" ← 这是触发器！                               │
│    正确：                                                        │
│    "根因是系统无法处理正常流量波动，缺少弹性伸缩机制"            │
│                                                                  │
└──────────────────────────────────────────────────────────────────┘
```
<!-- /DIAGRAM -->

### 1.4 Five Whys 模板

```markdown
# Five Whys 分析

**事故编号:** INC-2026-0110-001
**分析日期:** 2026-01-10
**分析人员:** [你的名字]

---

## 症状描述

[描述观察到的问题，包括时间、影响范围、用户反馈]

---

## Five Whys 分析

**Why 1:** 为什么 [症状] 发生？
  → [答案 1]

**Why 2:** 为什么 [答案 1]？
  → [答案 2]

**Why 3:** 为什么 [答案 2]？
  → [答案 3]

**Why 4:** 为什么 [答案 3]？
  → [答案 4]

**Why 5:** 为什么 [答案 4]？
  → [答案 5]

---

## 根因总结

**根本原因（系统/流程层面）:**
[描述根本原因]

**触发器（直接原因）:**
[描述触发事件]

---

## 注意事项

- 根因应该在系统或流程层面，而不是"某人犯了错"
- 如果根因是"人为失误"，继续问"什么流程允许这个错误发生？"
```

### 1.5 RCA 反模式

| 反模式 | 错误做法 | 后果 | 正确做法 |
|--------|----------|------|----------|
| **停在症状** | "根因是 500 错误" | 症状会再发生 | 问"为什么会 500？" |
| **混淆触发器和根因** | "根因是流量激增" | 没有解决真正问题 | 问"为什么系统无法应对流量？" |
| **指责个人** | "根因是小王配错了" | 人员防御，问题隐藏 | 问"什么流程允许这个错误发生？" |
| **只问一次** | "磁盘满了，清理一下" | 问题再发 | 继续问"为什么会满？" |
| **假设答案** | "肯定是网络问题" | 方向错误 | 用证据验证每一步 |

### 1.6 何时 Five Whys 不够用

Five Whys 适合线性因果链。对于复杂系统故障，可能需要更高级的方法：

| 情况 | Five Whys | 替代方法 |
|------|-----------|----------|
| 单一因果链 | 适用 | - |
| 多个独立根因 | 对每个分支做 Five Whys | 故障树分析 (FTA) |
| 复杂交互 | 不足 | 系统动力学 |
| 人因分析 | 基础 | HFACS, Swiss Cheese Model |

对于 Capstone 项目，Five Whys 足够使用。

---

## Step 2 -- 障害報告書 结构（30 分钟）

### 2.1 为什么需要正式报告？

在日本 IT 企业，故障报告（障害報告書）是**必须**的交付物：

- **责任追溯**：记录谁做了什么决定
- **知识沉淀**：防止同类问题再发
- **合规要求**：很多行业有监管要求
- **客户沟通**：对外解释需要正式文档

> **日本职场提示**：  
>
> 「障害報告書は日本企業で必須です。口頭報告だけでは不十分です。」  
>
> （障害报告书在日本企业是必须的。仅口头报告是不够的。）  

### 2.2 障害報告書 完整模板

```markdown
# 障害報告書

**報告書番号:** INC-2026-0110-001
**作成日:** 2026-01-10
**作成者:** [担当者名]
**承認者:** [上長名]

---

## 1. 概要

| 項目 | 内容 |
|------|------|
| 発生日時 | 2026-01-10 03:00 (JST) |
| 検知日時 | 2026-01-10 03:02 (JST) |
| 復旧日時 | 2026-01-10 04:15 (JST) |
| 影響時間 | 1時間15分 |
| 影響範囲 | 本番 API サーバー（全ユーザー影響） |
| 影響程度 | Critical（サービス完全停止） |

### 影響サマリ

- 影響ユーザー数: 約 10,000 人
- 発生エラー数: 約 50,000 件
- 業務影響: 注文処理が完全停止
- SLA 影響: 99.9% → 99.85%（月間目標未達）

---

## 2. 時系列 (Timeline)

| 時刻 (JST) | 事象 | 対応者 |
|------------|------|--------|
| 03:00 | API レスポンスタイム悪化開始 | - |
| 03:02 | 監視アラート発報（Response Time > 5s） | 監視システム |
| 03:05 | 担当者 A、アラート確認、調査開始 | 担当者 A |
| 03:10 | 証拠収集スクリプト実行 | 担当者 A |
| 03:15 | ディスク I/O wait 高騰を確認 | 担当者 A |
| 03:20 | 上長 B にエスカレーション | 担当者 A |
| 03:25 | ストレージチームに連絡 | 上長 B |
| 03:30 | ストレージ障害を特定 | ストレージチーム |
| 03:45 | 暫定対応：問題ディスク切り離し | ストレージチーム |
| 04:00 | API サービス再起動 | 担当者 A |
| 04:10 | 動作確認、レスポンス正常化 | 担当者 A |
| 04:15 | 正常復旧宣言 | 上長 B |

---

## 3. 根本原因 (Root Cause)

### 3.1 Five Whys 分析

**症状:** API が 5xx エラーを返し、レスポンスタイムが 5 秒以上に悪化

**Why 1:** 為什麼 API 返回 5xx 錯誤？
  → データベースへの書き込みがタイムアウト

**Why 2:** 為什麼資料庫寫入超時？
  → ストレージ I/O が極端に遅延

**Why 3:** 為什麼儲存 I/O 延遲？
  → RAID アレイの 1 ディスクに不良セクタ発生、リビルド開始

**Why 4:** 為什麼 RAID 重建影響這麼大？
  → 単一ストレージプールに全サービスが依存、I/O 分離なし

**Why 5:** 為什麼沒有 I/O 隔離？
  → コスト削減でストレージ設計を簡略化、リスク評価が不十分

### 3.2 根因総括

| 分類 | 内容 |
|------|------|
| 直接原因（触発器） | RAID ディスクの不良セクタ発生 |
| 根本原因（技術） | ストレージ I/O 分離設計の欠如 |
| 根本原因（プロセス） | 新規サービス導入時のストレージ容量/性能レビュー不足 |

---

## 4. 寄与要因 (Contributing Factors)

根本原因の他に、事態を悪化させた要因：

| 寄与要因 | 影響 |
|----------|------|
| ストレージ監視の閾値が高すぎ | 早期警告なし、発覚が遅れた |
| 障害時 Runbook がない | 対応に時間がかかった |
| RAID 健全性の定期チェックなし | 事前に検知できなかった |

---

## 5. 対応内容

### 5.1 暫定対応 (Workaround)

**目的:** サービスを最速で復旧させる一時的措置

| 対応 | 実施者 | 時刻 |
|------|--------|------|
| 問題ディスクを RAID アレイから切り離し | ストレージチーム | 03:45 |
| API サービス再起動 | 担当者 A | 04:00 |
| キャッシュクリア | 担当者 A | 04:05 |

**リスク:** RAID 冗長性が一時的に低下（1 ディスク減）

### 5.2 恒久対策 (Permanent Fix)

**目的:** 根本原因を解消し、再発を防止する

| 対策 | 担当 | 予定日 |
|------|------|--------|
| 問題ディスク交換、RAID リビルド完了 | ストレージチーム | 2026-01-11 |
| 本番データベース専用ストレージプール構築 | インフラチーム | 2026-01-31 |
| ストレージ性能設計ガイドライン策定 | アーキテクトチーム | 2026-02-15 |

---

## 6. 再発防止策 (Prevention Measures)

### 6.1 アクションアイテム

| # | 対策 | 担当者 | 期限 | 状態 |
|---|------|--------|------|------|
| 1 | ストレージ I/O 監視閾値見直し（50% → 30%） | 監視チーム | 2026-01-17 | 未着手 |
| 2 | RAID 健全性週次チェックジョブ追加 | ストレージチーム | 2026-01-17 | 未着手 |
| 3 | 本番 DB 専用ストレージプール構築 | インフラチーム | 2026-01-31 | 未着手 |
| 4 | ストレージ障害 Runbook 作成 | 運用チーム | 2026-01-24 | 未着手 |
| 5 | 新規サービスストレージレビュー必須化 | PM チーム | 2026-02-01 | 未着手 |

### 6.2 再発防止策の注意事項

**具体的であること**：

| NG | OK |
|----|----|
| 「気をつける」 | 「チェックリストに項目追加」 |
| 「注意喚起する」 | 「全員に研修実施、受講記録残す」 |
| 「監視を強化」 | 「閾値を 50% から 30% に変更」 |

> **日本職場提示**：  
>
> 「再発防止策は具体的に。"気をつける" は NG です。」  
>
> （再发防止措施要具体。"注意一下"是不行的。）  

---

## 7. 教訓 (Lessons Learned)

### 7.1 うまくいったこと

- 証拠収集スクリプトを事前に準備していたため、迅速に状況把握できた
- エスカレーションが適切なタイミングで行われた
- チーム間連携がスムーズだった

### 7.2 改善が必要なこと

- ストレージ監視が不十分だった
- 障害時 Runbook がなく、対応に時間がかかった
- 根本的なアーキテクチャリスクが見落とされていた

---

## 8. 承認

| 役職 | 氏名 | 日付 | 署名 |
|------|------|------|------|
| 作成者 | | | |
| レビュー者 | | | |
| 承認者 | | | |

---

**添付資料:**
- 証拠収集スクリプト出力
- 監視アラートログ
- Five Whys 分析ワークシート
```

### 2.3 暫定対応 vs 恒久対策

这是日本 IT 报告中的关键区分：

<!-- DIAGRAM: workaround-vs-permanent -->
```
┌──────────────────────────────────────────────────────────────────┐
│                暫定対応 vs 恒久対策                               │
├──────────────────────────────────────────────────────────────────┤
│                                                                  │
│    暫定対応 (Workaround)         恒久対策 (Permanent Fix)        │
│    ┌─────────────────────┐       ┌─────────────────────┐        │
│    │ 目的：快速恢复服务   │       │ 目的：消除根因       │        │
│    │ 时效：临时，有风险   │       │ 时效：永久           │        │
│    │ 验证：基本功能测试   │       │ 验证：完整回归测试   │        │
│    └──────────┬──────────┘       └──────────┬──────────┘        │
│               │                              │                   │
│               ▼                              ▼                   │
│    ┌─────────────────────┐       ┌─────────────────────┐        │
│    │ 例子：               │       │ 例子：               │        │
│    │ • 重启服务           │       │ • 修复代码 bug       │        │
│    │ • 切换到备用节点     │       │ • 重新设计架构       │        │
│    │ • 手动清理磁盘       │       │ • 配置自动化         │        │
│    │ • 禁用问题功能       │       │ • 添加监控告警       │        │
│    └─────────────────────┘       └─────────────────────┘        │
│                                                                  │
│    重要：两者都要记录！                                           │
│    • 暫定対応 解决了紧急问题                                      │
│    • 恒久対策 防止问题再发                                        │
│    • 如果只有暫定対応，问题会再发生                               │
│                                                                  │
└──────────────────────────────────────────────────────────────────┘
```
<!-- /DIAGRAM -->

---

## Step 3 -- Blameless 文化（20 分钟）

### 3.1 什么是 Blameless Postmortem？

Blameless（无责）文化源自 Google SRE 实践，核心理念是：

> **关注流程改进，而不是追究个人责任。**  

<!-- DIAGRAM: blameless-culture -->
```
┌──────────────────────────────────────────────────────────────────┐
│                    Blameless vs Blame 文化                        │
├──────────────────────────────────────────────────────────────────┤
│                                                                  │
│   Blame 文化                       Blameless 文化                │
│   ┌─────────────────────┐         ┌─────────────────────┐       │
│   │ "谁搞砸的？"        │         │ "什么流程失败了？"  │       │
│   │ "谁要负责？"        │         │ "如何防止再发？"    │       │
│   │ "这是谁批准的？"    │         │ "我们能改进什么？"  │       │
│   └──────────┬──────────┘         └──────────┬──────────┘       │
│              │                               │                   │
│              ▼                               ▼                   │
│   ┌─────────────────────┐         ┌─────────────────────┐       │
│   │ 结果：               │         │ 结果：               │       │
│   │ • 人员隐瞒问题       │         │ • 问题被主动暴露     │       │
│   │ • 不敢承担风险       │         │ • 创新和实验增加     │       │
│   │ • 防御性文档         │         │ • 真诚的知识共享     │       │
│   │ • 问题反复发生       │         │ • 持续改进           │       │
│   └─────────────────────┘         └─────────────────────┘       │
│                                                                  │
│   核心转变：                                                     │
│   "小王配错了配置" → "配置审查流程缺失"                          │
│   "开发写了 bug"   → "代码审查和测试覆盖不足"                    │
│   "运维没有监控"   → "监控覆盖标准未定义"                        │
│                                                                  │
└──────────────────────────────────────────────────────────────────┘
```
<!-- /DIAGRAM -->

### 3.2 Blameless 的核心原则

1. **假设每个人都在尽力工作**
   - 人会犯错，这是正常的
   - 关注"系统如何允许错误发生"

2. **关注系统和流程**
   - 一个好的系统应该能容忍人为错误
   - 如果一个错误能导致故障，系统需要改进

3. **创造心理安全**
   - 人们愿意主动报告问题
   - 不怕被惩罚，才会说真话

4. **将故障视为学习机会**
   - 每次故障都是改进的机会
   - 比起惩罚，更重要的是防止再发

### 3.3 Blameless 语言转换

| Blame 语言 | Blameless 语言 |
|------------|----------------|
| "小王忘了配置 logrotate" | "部署检查清单缺少日志配置验证" |
| "测试没有发现这个 bug" | "测试用例覆盖不足，缺少边界条件测试" |
| "运维没有注意到告警" | "告警太多导致疲劳，需要优化告警策略" |
| "为什么没人检查？" | "检查流程没有定义或没有强制执行" |

### 3.4 日本 IT 文化中的 Blameless

> **注意**：日本传统企业文化偏向责任追究。但现代 IT 企业（特别是外资和初创公司）  
> 正在采用 Blameless 实践。了解两种文化，根据公司氛围调整。  

| 传统日本企业 | 现代 IT 企业 |
|-------------|-------------|
| 「責任者を明確に」| 「プロセス改善を重視」|
| 書面謝罪を求める | 改善計画を求める |
| 個人評価に影響 | 学習機会として扱う |

在撰写 障害報告書 时，即使公司文化偏传统，也应该：
- 在"根本原因"部分聚焦流程
- 避免直接指名个人
- 强调"系统改进"而非"人员教育"

---

## Step 4 -- Action Item 管理（15 分钟）

### 4.1 有效的 Action Item

再発防止策 必须是：

| 特征 | 说明 | 例子 |
|------|------|------|
| **具体** | 明确说明要做什么 | "添加磁盘使用率 80% 告警" |
| **可衡量** | 能验证是否完成 | "监控配置上线，告警触发测试通过" |
| **有责任人** | 明确谁负责 | "监控团队 - 张三" |
| **有截止日期** | 明确什么时候完成 | "2026-01-17" |
| **可跟踪** | 定期复查进度 | "每周站会检查" |

### 4.2 Action Item 反模式

| 反模式 | 例子 | 问题 | 改进 |
|--------|------|------|------|
| 太模糊 | "加强监控" | 什么监控？怎么加强？ | "添加 CPU > 80% 告警到 Prometheus" |
| 无责任人 | "需要改进部署流程" | 谁来做？ | "DevOps 团队 - 李四负责" |
| 无截止日 | "尽快完成" | 什么时候算"尽快"？ | "2026-01-24 之前" |
| 重复工作 | "注意配置" | 没有系统改进 | "配置审查 checklist 并自动化验证" |

### 4.3 Action Item 跟踪模板

```markdown
# 再発防止策 追跡表

**事故編号:** INC-2026-0110-001
**最終更新:** 2026-01-10

## アクションアイテム

| # | 対策 | 担当 | 期限 | 状態 | 完了日 | 備考 |
|---|------|------|------|------|--------|------|
| 1 | ストレージ監視閾値変更 | 監視チーム/張 | 01-17 | ✅ 完了 | 01-12 | PR #123 |
| 2 | RAID 健全性チェック | ストレージ/李 | 01-17 | 🔄 進行中 | - | cron 設定中 |
| 3 | 専用ストレージ構築 | インフラ/王 | 01-31 | ⏳ 未着手 | - | 予算承認待ち |
| 4 | Runbook 作成 | 運用/田中 | 01-24 | ⏳ 未着手 | - | - |

## 週次レビュー記録

### 2026-01-17 週次会議
- Item #1: 完了確認
- Item #2: cron 設定完了、テスト中
- Item #3: 予算承認済み、作業開始
- Item #4: 次週着手予定

## 状態凡例
- ⏳ 未着手
- 🔄 進行中
- ✅ 完了
- ❌ 取消
- 🚧 ブロック中
```

---

## Step 5 -- Capstone 项目：级联故障（60 分钟）

### 5.1 场景介绍

你将面对一个真实的多系统级联故障场景：

<!-- DIAGRAM: cascade-failure -->
```
┌──────────────────────────────────────────────────────────────────┐
│                    级联故障场景                                   │
├──────────────────────────────────────────────────────────────────┤
│                                                                  │
│   触发器                                                         │
│   ┌─────────────┐                                                │
│   │ Storage     │   磁盘出现坏扇区，I/O 延迟飙升                 │
│   │ Slow        │                                                │
│   └──────┬──────┘                                                │
│          │                                                       │
│          ▼                                                       │
│   ┌─────────────┐                                                │
│   │ Service     │   数据库查询超时，API 响应变慢                 │
│   │ Timeout     │                                                │
│   └──────┬──────┘                                                │
│          │                                                       │
│          ▼                                                       │
│   ┌─────────────┐                                                │
│   │ Retry       │   客户端重试，请求量翻倍                       │
│   │ Storm       │                                                │
│   └──────┬──────┘                                                │
│          │                                                       │
│          ▼                                                       │
│   ┌─────────────┐                                                │
│   │ CPU         │   处理大量重试请求，CPU 100%                   │
│   │ Spike       │                                                │
│   └─────────────┘                                                │
│                                                                  │
│   症状：                                                         │
│   • 多个告警同时触发（Storage、DB、API、CPU）                    │
│   • 难以判断哪个是根因                                           │
│   • 需要按正确顺序处理                                           │
│                                                                  │
└──────────────────────────────────────────────────────────────────┘
```
<!-- /DIAGRAM -->

### 5.2 场景详情

**时间**：2026-01-10 03:00 JST

**告警**（按触发顺序）：
```
03:00:00 [WARNING]  storage-01: Disk I/O await > 200ms
03:00:30 [WARNING]  db-master: Query latency > 1s
03:01:00 [CRITICAL] api-server: Response time > 5s
03:01:30 [CRITICAL] api-server: 5xx error rate > 50%
03:02:00 [CRITICAL] api-server: CPU usage > 95%
```

**你有的资源**：
- SSH 访问所有服务器
- 证据采集脚本 (`collect-evidence.sh`)
- 监控系统 (Prometheus + Grafana)
- 团队 Slack 频道

### 5.3 Capstone 任务清单

#### 技术任务

1. **证据采集**
   - 在修复前运行证据采集脚本
   - 保存所有相关日志和指标
   - 记录当前系统状态

2. **问题诊断**
   - 使用 USE/RED 方法论
   - 确定故障传播链
   - 找到真正的根因

3. **服务恢复**
   - 实施暫定対応
   - 验证服务恢复
   - 记录恢复时间

#### 文档任务

4. **Five Whys 分析**
   - 完成完整的 Five Whys 工作表
   - 明确区分触发器和根因

5. **障害報告書**
   - 使用完整模板
   - 包含所有必需部分
   - 提出具体的再発防止策

### 5.4 模拟环境搭建

如果你有 VM 或云环境，可以模拟这个场景：

```bash
# 在存储服务器上模拟 I/O 延迟
# 注意：这会真的影响性能，仅用于测试环境！

# 方法1：使用 tc 模拟块设备延迟（需要 dm-delay）
# 这是高级操作，建议在虚拟机中测试

# 方法2：使用 stress-ng 产生 I/O 负载
stress-ng --hdd 4 --hdd-bytes 1G --timeout 300s

# 在应用服务器上观察
watch -n1 'iostat -xz 1 1; echo "---"; top -bn1 | head -10'
```

如果没有测试环境，可以基于以下模拟数据完成报告：

### 5.5 模拟数据（用于报告编写）

**存储服务器 (`storage-01`) 证据：**
```
# dmesg 输出
[  123.456789] sd 0:0:0:0: [sda] Add. Sense: Unrecovered read error
[  123.567890] blk_update_request: I/O error, dev sda, sector 12345678

# iostat -xz 1 输出
Device  r/s   w/s  await  %util
sda     15.0  45.0  450.2  98.5   ← await 正常 < 20ms，现在 450ms
sdb     2.0   1.0   5.2    3.1

# smartctl -a /dev/sda 输出
SMART Attributes:
  5 Reallocated_Sector_Ct   0x0033   100   100   005    Pre-fail   64   ← 坏扇区
197 Current_Pending_Sector  0x0012   100   100   000    Old_age    8    ← 待处理扇区
```

**数据库服务器 (`db-master`) 证据：**
```
# 慢查询日志
# Time: 2026-01-10T03:00:15.123456Z
# Query_time: 5.234567  Lock_time: 0.000123  Rows_examined: 10000
SELECT * FROM orders WHERE created_at > '2026-01-09';

# processlist
+----+------+-----------+-----+---------+------+-------+------------------+
| Id | User | Host      | db  | Command | Time | State | Info             |
+----+------+-----------+-----+---------+------+-------+------------------+
| 42 | app  | api-01    | prod| Query   | 5    | Sending| SELECT * FROM... |
| 43 | app  | api-01    | prod| Query   | 4    | Sending| SELECT * FROM... |
| 44 | app  | api-01    | prod| Query   | 3    | Sending| SELECT * FROM... |
+----+------+-----------+-----+---------+------+-------+------------------+
← 大量查询堆积
```

**API 服务器 (`api-01`) 证据：**
```
# 应用日志
03:00:30 ERROR Database query timeout after 5000ms
03:00:31 WARN  Retrying request (attempt 2/3)
03:00:32 ERROR Database query timeout after 5000ms
03:00:33 WARN  Retrying request (attempt 3/3)
03:00:34 ERROR Request failed after 3 retries

# top 输出
top - 03:02:00 up 45 days, 3:21, 1 user, load average: 45.23, 12.34, 5.67
                                                       ↑ 飙升
%Cpu(s): 95.2 us,  3.1 sy,  0.0 ni,  0.0 id,  1.2 wa,  0.0 hi,  0.5 si

# ss -s 输出
Total: 15234 (kernel 0)
TCP:   12345 (estab 8765, closed 2345, orphaned 123, timewait 1112)
       ↑ 连接数异常高
```

### 5.6 评分标准

| 评分项 | 权重 | 评分标准 |
|--------|------|----------|
| **正确识别根因** | 30% | 找到存储问题是级联故障的起点；区分触发器（坏扇区）和根因（无 I/O 隔离） |
| **时间线完整准确** | 20% | 包含所有关键事件；时间戳使用 JST；分钟级精度 |
| **障害報告書质量** | 30% | 使用完整模板；内容清晰准确；日语术语正确使用 |
| **再発防止措施具体可行** | 20% | 每项有责任人和截止日；措施具体可验证；不含"注意一下" |

---

## 动手实验：完成 Capstone（60 分钟）

### 实验目标

基于上述场景和模拟数据，完成以下交付物：

### 交付物 1：Five Whys 分析工作表

```markdown
# Five Whys 分析

**事故编号:** INC-2026-0110-001
**分析日期:** [填写]
**分析人员:** [你的名字]

---

## 症状描述

[填写：描述观察到的问题]

---

## Five Whys 分析

**Why 1:** 为什么 API 返回 5xx 错误并且响应超时？
  → [填写]

**Why 2:** 为什么 [Why 1 的答案]？
  → [填写]

**Why 3:** 为什么 [Why 2 的答案]？
  → [填写]

**Why 4:** 为什么 [Why 3 的答案]？
  → [填写]

**Why 5:** 为什么 [Why 4 的答案]？
  → [填写]

---

## 根因总结

**触发器（直接原因）:**
[填写]

**根本原因（系统/流程层面）:**
[填写]
```

### 交付物 2：完整的 障害報告書

使用 Step 2 中的模板，填写完整的报告。

### 参考答案

<details>
<summary>点击查看 Five Whys 参考答案</summary>

**Why 1:** 为什么 API 返回 5xx 错误并且响应超时？
  → 因为数据库查询超时，应用无法获取数据

**Why 2:** 为什么数据库查询超时？
  → 因为存储 I/O 延迟极高（await > 400ms），查询等待磁盘

**Why 3:** 为什么存储 I/O 延迟高？
  → 因为磁盘出现坏扇区，RAID 控制器在进行错误处理

**Why 4:** 为什么磁盘坏扇区影响这么大？
  → 因为所有服务共享同一存储池，没有 I/O 隔离

**Why 5:** 为什么没有 I/O 隔离？
  → 因为成本优化时简化了存储架构，风险评估不足

**触发器：** 磁盘物理故障（坏扇区）

**根本原因：**
- 技术层面：关键服务缺少独立存储池，无 I/O 隔离
- 流程层面：新服务上线缺少存储容量/性能评审

</details>

---

## 检查清单

完成本课后，你应该能够：

- [ ] 应用 Five Whys 方法进行根因分析
- [ ] 区分触发器和根因
- [ ] 理解 Blameless 文化的核心原则
- [ ] 编写完整的 障害報告書
- [ ] 区分 暫定対応 和 恒久対策
- [ ] 制定具体、可衡量、有责任人的 Action Items
- [ ] 完成 Capstone 项目的所有交付物
- [ ] 使用正确的日语术语描述故障处理流程

---

## 本课小结

| 概念 | 要点 |
|------|------|
| Five Whys | 连续问"为什么"直到系统/流程层面 |
| 触发器 vs 根因 | 触发器是事件，根因是系统缺陷 |
| Blameless 文化 | 关注流程改进，不追究个人 |
| 障害報告書 | 时间线 + 根因 + 暫定対応 + 恒久対策 + 再発防止 |
| 暫定対応 | 临时恢复服务的措施 |
| 恒久対策 | 消除根因的永久修复 |
| 再発防止 | 具体、可衡量、有责任人、有截止日 |

**核心理念：**

> 故障排查不是"修好了就完了"，而是"让它不再发生"。  
>
> 一份好的 RCA 报告，价值超过任何技术修复。  

---

## 面试准备

### よくある質問（常见问题）

**Q: Five Whys 分析とは何ですか？**

A: Five Whys はトヨタ生産方式で開発された根本原因分析手法です。「なぜ」を 5 回繰り返すことで、表面的な症状から根本原因まで掘り下げます。重要なのは、個人の責任ではなく、システムやプロセスレベルの原因で止めることです。

**Q: 障害報告書に何を含めますか？**

A: 主要な要素は：
1. **概要**：発生時刻、復旧時刻、影響範囲
2. **時系列**：分単位の対応履歴
3. **根本原因**：Five Whys 分析結果
4. **対応内容**：暫定対応と恒久対策
5. **再発防止策**：具体的なアクションアイテム

**Q: 暫定対応と恒久対策の違いは？**

A:
- **暫定対応（ざんていたいおう）**：サービスを素早く復旧させる一時的な対策。例：サービス再起動、フェイルオーバー
- **恒久対策（こうきゅうたいさく）**：根本原因を解消する永続的な修正。例：コード修正、アーキテクチャ変更

**Q: Blameless Postmortem とは？**

A: 個人の責任追及ではなく、プロセス改善に焦点を当てる事後分析手法です。Google SRE が提唱しました。心理的安全性を確保し、問題の隠蔽を防ぎ、組織全体の学習を促進します。

**Q: 再発防止策のポイントは？**

A: 具体的で測定可能であること。「気をつける」「注意喚起」は NG です。
良い例：「デプロイチェックリストにログローテーション確認項目を追加（担当：田中、期限：1/17）」

---

## トラブルシューティング（本課自体の問題解決）

### Five Whys で行き詰まった

**症状**：3 回目の Why で答えが出なくなる

**解決策**：
1. 証拠を見直す - 推測ではなく事実に基づく
2. 複数の分岐を試す - 一つの答えに固執しない
3. チームでブレインストーミング - 他の視点を得る

### 暫定対応が見つからない

**症状**：根本修正しか思いつかない

**解決策**：
1. サービス再起動で一時的に解決しないか
2. 問題の機能を無効化できないか
3. 負荷を別のシステムに逃がせないか
4. キャッシュやリードレプリカで回避できないか

### 再発防止策が抽象的になる

**症状**：「監視を強化する」のような曖昧な表現になる

**解決策**：
- 「何を」「どう」「誰が」「いつまでに」を明確に
- 完了条件を定義する（「〇〇アラートが Prometheus に追加され、テストで発報確認」）
- 「気をつける」を禁止語にする

---

## 延伸阅读

- [Google SRE Book - Postmortem Culture](https://sre.google/sre-book/postmortem-culture/)
- [Etsy's Blameless PostMortems](https://codeascraft.com/2012/05/22/blameless-postmortems/)
- [The Five Whys - Toyota](https://blog.toyota.co.uk/the-five-whys)
- [PagerDuty Incident Response](https://response.pagerduty.com/)
- 上一课：[09 - Core Dump 与崩溃分析](../09-core-dumps/)
- [系列首页](../)

---

## 恭喜完成！

你已经完成了 **LX10-TROUBLESHOOTING** 全部 10 课内容！

通过这个课程，你学会了：

1. **方法论**：USE/RED 框架驱动的系统性排查
2. **启动故障**：GRUB、initramfs、Emergency Mode
3. **服务故障**：systemd 依赖分析
4. **网络问题**：分层诊断 L3→L4→L7
5. **存储问题**：容量、inode、I/O 错误
6. **性能分析**：Load Average、I/O wait、OOM
7. **日志分析**：journalctl 精通、时间线重建
8. **strace 调试**：系统调用追踪
9. **Core Dump**：崩溃分析
10. **RCA**：Five Whys、障害報告書

**下一步建议：**
- 在实际工作中应用这些技能
- 积累自己的故障案例库
- 考虑 RHCE 或 LPIC-3 认证

---

## 系列导航

[<-- 09 - Core Dump 与崩溃分析](../09-core-dumps/) | [系列首页](../)
