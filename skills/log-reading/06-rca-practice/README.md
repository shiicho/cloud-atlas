# 06 · RCA 根因分析实战（Five Whys + 再発防止）

> **目标**：运用 Five Whys 方法论深挖根因，制定有效的再発防止策  
> **前置**：[05 · 故障时间线重建](../05-timeline-report/)  
> **区域**：任意  
> **费用**：无额外费用

## 将完成的内容

1. 理解 Five Whys 方法论
2. 避免 RCA 常见陷阱
3. 制定有效的再発防止策
4. 实战：电商促销日多阶段故障 RCA
5. 输出完整障害報告書

---

## Five Whys 方法论 {#five-whys}

### 什么是 Five Whys

Five Whys 是一种通过连续追问「为什么」来发现根本原因的方法。

目标：从表面症状追溯到系统性/流程性问题。

### 示例

```
症状: checkout 超时

Why 1: 为什么 checkout 超时？
→ 因为 Redis 响应慢

Why 2: 为什么 Redis 响应慢？
→ 因为 Redis 正在执行 RDB save

Why 3: 为什么 RDB save 影响性能？
→ 因为内存不足，save 时触发了 swap

Why 4: 为什么内存不足？
→ 因为流量激增，缓存膨胀

Why 5: 为什么没有容量预警？
→ 因为没有针对促销日的容量评审流程

根因: 缺乏容量评审流程
```

### Five Whys 原则

| 原则 | 说明 |
|------|------|
| **追问到系统层** | 不要停在「人的错误」，要追到流程/设计层 |
| **避免责怪个人** | 「因为他忘了」不是根因，要问「为什么会忘」 |
| **可以分支** | 一个问题可能有多个原因 |
| **至少 5 层** | 通常需要 5 层才能到达根因 |
| **可验证** | 根因应该可以通过证据验证 |

---

## RCA 常见陷阱

### 陷阱 1: 停在症状层

**错误示例**：
```
问题: 用户报告 500 错误
根因: 服务器返回了 500

❌ 这只是症状复述，不是根因
```

**正确做法**：继续追问为什么返回 500

### 陷阱 2: 责怪个人

**错误示例**：
```
Why 3: 为什么配置错误？
答: 因为运维人员粗心

❌ 停在个人层面
```

**正确做法**：
```
Why 3: 为什么配置错误？
答: 因为变更没有经过 review

Why 4: 为什么没有 review？
答: 因为没有强制 review 流程

✓ 追到流程层面
```

### 陷阱 3: 第一层就停止

**错误示例**：
```
问题: Redis OOM
根因: 内存不足
解决: 加内存

❌ 没有追问为什么内存不足
```

**正确做法**：追问内存不足的原因，可能发现是缓存策略、容量规划、流量激增等深层问题

### 陷阱 4: 混淆触发因素和根因

```
触发因素: 促销日流量激增
根因: 缺乏容量评审流程

区别:
- 触发因素是「点燃导火索的火」
- 根因是「导火索本身存在」
```

---

## 再発防止策制定 {#prevention}

### 有效 vs 无效的对策

| 无效对策 | 有效对策 |
|---------|---------|
| 「注意する」「気をつける」 | 添加自动化检查 |
| 「教育を強化」 | 添加监控告警 |
| 「マニュアル作成」 | 引入 CI/CD 验证 |
| 「レビュー依頼」 | 强制 review + block |

### 对策分类

| 类别 | 示例 | 效果 |
|------|------|------|
| **检测** | 添加监控告警 | 快速发现 |
| **预防** | 自动化检查、限流 | 避免发生 |
| **响应** | 自动回滚、failover | 减少影响 |
| **流程** | review 必须化、变更冻结 | 长期防止 |

### 对策模板

```markdown
| 项目 | 对策 | 担当 | 期限 |
|------|------|------|------|
| 检测 | Redis 内存使用率 >80% 告警 | インフラ | 2024-07-05 |
| 预防 | 促销前容量评审必须化 | 开发/运营 | 2024-07-10 |
| 响应 | Redis maxmemory-policy 设为 volatile-lru | インフラ | 2024-07-02 |
| 流程 | 大型活动前变更冻结 | チーム | 即时 |
```

---

## 实战练习：电商促销日多阶段故障 RCA

### 场景描述

7月1日 20:00（JST），电商网站 checkout 功能大面积超时。需要进行完整 RCA。

### 日志样本

**Nginx (+0900 JST)：**
```
[01/Jul/2024:20:00:05 +0900] "POST /checkout" 504 182 "-" "Chrome"
```

**App (UTC)：**
```
2024-07-01T11:00:05Z WARN checkout timeout order=8891
```

**Redis（无日期，只有时间）：**
```
20:00:03.123 * 10000 changes in 300 seconds. Saving...
20:00:08.555 # Background saving terminated by signal 9
```

**Kernel：**
```
Jul  1 20:00:08 ip-10-0-2-5 kernel: Out of memory: Kill process 4321 (redis-server) score 995 or sacrifice child
```

**RDS Slow Query (UTC)：**
```
2024-07-01T10:59:55Z SELECT ... FOR UPDATE rows=1 time=3.2s
```

**CloudTrail (UTC)：**
```
2024-07-01T10:50:00Z ModifyDBInstance m5.large->m6g.large (pending) user=admin
```

**ALB：**
```
172.31.20.10:443 10.0.2.10:53211 0.000 5.002 0.000 504 504 0 57 "POST https://shop.example.com/checkout"
```

### Step 1: 统一时区，建立时间线

| 原始时间 | 来源 | JST 时间 |
|---------|------|---------|
| 2024-07-01T10:50:00Z | CloudTrail | 19:50:00 |
| 2024-07-01T10:59:55Z | RDS | 19:59:55 |
| 20:00:03 | Redis | 20:00:03 (假定 JST) |
| 2024-07-01T11:00:05Z | App | 20:00:05 |
| [01/Jul/2024:20:00:05 +0900] | Nginx | 20:00:05 |
| Jul 1 20:00:08 | Kernel | 20:00:08 |
| 20:00:08.555 | Redis | 20:00:08 |

**时间线：**
```
19:50:00 - CloudTrail: ModifyDBInstance pending
19:59:55 - RDS: Slow query 3.2s
20:00:03 - Redis: RDB save 开始
20:00:05 - App: checkout timeout
20:00:05 - Nginx: 504 错误
20:00:08 - Kernel: OOM kill redis-server
20:00:08 - Redis: Background saving terminated
```

### Step 2: Five Whys 分析

```
症状: checkout 504 超时

Why 1: 为什么 checkout 超时？
→ 因为 Redis 响应慢/被 OOM kill

Why 2: 为什么 Redis 被 OOM kill？
→ 因为内存不足（score 995 表示内存使用极高）

Why 3: 为什么内存不足？
→ 因为 RDB save 占用额外内存 + 流量激增

Why 4: 为什么 RDB save 在高峰期触发？
→ 因为默认配置「10000 changes in 300 seconds」触发

Why 5: 为什么高峰期内存不足？
→ 因为促销日流量预估不足 + DB 变更导致查询变慢，更依赖缓存

Why 6: 为什么 DB 变更在促销日？
→ 因为没有变更冻结机制
```

### Step 3: 根因链分析

```
┌─────────────────────────────────────────────────────────────┐
│                     根因链（因果关系）                        │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  [19:50] DB 改规格 pending                                   │
│      ↓                                                      │
│  [19:59] 查询变慢 (3.2s)                                     │
│      ↓                                                      │
│  App 更依赖 Redis cache                                      │
│      ↓                                                      │
│  [20:00] Redis RDB save 触发（10000 changes/300s）           │
│      ↓                                                      │
│  内存压力增大                                                 │
│      ↓                                                      │
│  [20:00:08] OOM killer 杀掉 redis-server                     │
│      ↓                                                      │
│  缓存失效                                                    │
│      ↓                                                      │
│  checkout 超时 → 504                                         │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### Step 4: 评估盲区

| 盲区 | 发现方式 | 应有检测 |
|------|---------|---------|
| DB 变更 pending | CloudTrail | RDS 状态监控 |
| Redis 内存高 | 事后 dmesg | 内存使用率告警 |
| Slow query | RDS 日志 | 慢查询告警 |
| OOM 风险 | Kernel 日志 | 内存预警 |

### Step 5: 完整障害報告書

```markdown
# 障害報告書

## 件名/概要
[JST] 2024-07-01 20:00-20:25 チェックアウト機能504エラー多発

## 影響範囲
- 影響時間: 2024-07-01 20:00-20:25 JST (約25分間)
- 影響サービス: チェックアウト機能
- 影響リクエスト数: 約2,500件が504エラー
- 売上影響: 推定 ¥xxx万円

## 原因
複合的な要因による Redis OOM:

1. DB インスタンス変更が pending 状態で、クエリ性能低下
2. アプリがより多くの Redis キャッシュを使用
3. Redis RDB save が高負荷時に発動
4. メモリ不足で OOM killer が redis-server を終了
5. キャッシュ消失により checkout タイムアウト

## 時系列
| 時刻 (JST) | イベント |
|------------|----------|
| 19:50:00 | CloudTrail: ModifyDBInstance pending |
| 19:59:55 | RDS: Slow query 3.2s 発生 |
| 20:00:03 | Redis: RDB save 開始 |
| 20:00:05 | App: checkout timeout 開始 |
| 20:00:08 | Kernel: OOM kill redis-server |
| 20:10:00 | アラート受信、調査開始 |
| 20:15:00 | Redis 再起動 |
| 20:20:00 | DB 変更キャンセル、旧インスタンスに戻す |
| 20:25:00 | サービス正常復旧確認 |

## 対応
1. 20:10 - 監視アラート受信
2. 20:12 - Nginx 504 確認、Redis 疑い
3. 20:13 - dmesg で OOM 発見
4. 20:15 - Redis 再起動
5. 20:18 - CloudTrail で DB 変更発見
6. 20:20 - DB 変更キャンセル
7. 20:25 - 全サービス正常確認

## 再発防止策
| 項目 | 対策 | 担当 | 期限 |
|------|------|------|------|
| 検知 | Redis メモリ使用率 >80% 告警追加 | インフラ | 2024-07-05 |
| 検知 | RDS pending 状態監視追加 | インフラ | 2024-07-05 |
| 検知 | Slow query >1s 告警追加 | DBA | 2024-07-08 |
| 予防 | Redis maxmemory-policy を volatile-lru に変更 | インフラ | 2024-07-02 |
| 予防 | 促销前容量評価必須化 | 開発/運営 | 2024-07-15 |
| 流程 | 大型活動前 48h 変更凍結 | チーム | 即時 |

## 教訓
- 変更窗口とビジネスイベントの衝突を防ぐカレンダー管理必要
- Redis ログに日付がない問題（運用改善）
- 複合障害では多ソースログ相関が必須
```

---

## 评分标准（自我检查）

| 检查项 | 是否完成 |
|--------|---------|
| 是否提到变更窗口与促销日冲突 | |
| 是否指出 Redis 日志无日期需补当天 | |
| 是否正确处理 ALB/Nginx 时区差换算 | |
| 是否提出检测盲区（没有 OOM 预警/没有 RDS pending 监控） | |
| 再発防止是否具体（容量评审、变更冻结、慢查询阈值、Redis maxmemory policy） | |

---

## 面试常见问题

### Q1: Five Whys 要避免什么？

**期望回答**：
> - 避免责怪个人（「因为他粗心」不是根因）
> - 避免停在症状层（「因为 500 错误」不是根因）
> - 要追到系统/流程/设计层
> - 至少 5 层深度

**红旗回答**：
- 第一层就止步
- 指责个人

### Q2: 如何用日志确认 OOM root cause 而非瞬时 spike？

**期望回答**：
> 1. 查看事前内存增长趋势（如果有监控）
> 2. 查看 slow log 了解查询压力
> 3. 查看 RDB save 是否在高峰期触发
> 4. 关联其他系统变更（如 DB 变更）
>
> 如果只是瞬时 spike，不会有持续的慢查询和 RDB save 触发。

**红旗回答**：
- 看到 OOM 就结论「给更多内存」
- 不追问 OOM 的触发原因

---

## 常见错误

1. **停留在症状层（超时/5xx），不追到资源/配置/发布/容量**
   - 必须追问「为什么」

2. **Five Whys 第一层就停止**
   - 至少问 5 层

3. **再発防止写「加强培训」而非具体技术措施**
   - 要有可执行、可验证的对策

---

## 快速参考

| 需求 | 方法 |
|------|------|
| 追根因 | Five Whys（至少 5 层） |
| 避免责怪个人 | 追到系统/流程层 |
| 有效对策 | 检测/预防/响应/流程 |
| 无效对策 | 「注意」「培训」「下次小心」 |
| 时间线 | 统一时区，按 epoch 排序 |

---

## 课程总结

恭喜完成「日志分析与故障排查实战」全部 7 课！

### 技能总结

| 课程 | 技能 |
|------|------|
| 00 | Linux 日志架构（journalctl, dmesg, auth.log） |
| 01 | 工具链（grep/rg/jq/less）+ 异常识别 |
| 02 | systemd 服务分析（crash loop, timeout） |
| 03 | Web 服务器日志（5xx spike, top IP） |
| 04 | AWS 日志（CloudTrail, VPC Flow, ALB） |
| 05 | 时间线重建 + 障害報告書 |
| 06 | RCA Five Whys + 再発防止 |

### 下一步建议

- 在实际工作中应用这些技能
- 收集自己遇到的故障案例
- 练习撰写障害報告書
- 参与 on-call 值班积累经验

---

## 系列导航 / Series Nav

| 课程 | 主题 |
|------|------|
| [00 · Linux 日志系统概览](../00-linux-logs/) | journalctl, dmesg, auth.log |
| [01 · 日志分析工具与模式识别](../01-tools-patterns/) | grep/rg/jq/less |
| [02 · systemd 服务日志分析](../02-systemd-logs/) | crash loop, timeout |
| [03 · Web 服务器日志](../03-web-server-logs/) | Nginx/Apache 5xx |
| [04 · AWS 日志实战](../04-aws-logs/) | CloudTrail, VPC Flow |
| [05 · 故障时间线重建](../05-timeline-report/) | 障害報告書 |
| **06 · RCA 根因分析实战** | 当前（最后一课） |
